#!/usr/bin/env python3
"""
Lance un ou plusieurs batchs OpenAI Ã  partir de leur nom
(ex. python launch_batches.py retail_part2 legal_part3).
Les meta-fichiers _file_info.json doivent exister (gÃ©nÃ©rÃ©s par prepare_batches.py).
"""

import json
import sys
import time
from pathlib import Path
import tiktoken
from openai import OpenAI

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL = "gpt-4.1"
POLL_DELAY_SECONDS = 240
OUTPUT_DIR = Path("event-cot/openai_outputs")  # mÃªme dossier que prÃ©cÃ©demment

# â”€â”€ INIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    ENCODING = tiktoken.encoding_for_model(MODEL)
except KeyError:
    ENCODING = tiktoken.get_encoding("o200k_base")

client = OpenAI()
TERMINAL = {"completed", "failed", "cancelled", "expired"}

# â”€â”€ FONCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def wait(batch_id: str):
    while True:
        b = client.batches.retrieve(batch_id)
        status = b.status
        if status in TERMINAL:
            print(f"âœ… Batch {batch_id} terminÃ© â€” {status}")
            return status
        print(f"â³ Batch {batch_id} toujours {status}â€¦ (prochain check dans {POLL_DELAY_SECONDS}s)")
        time.sleep(POLL_DELAY_SECONDS)

def launch_one(batch_name: str):
    meta_path = OUTPUT_DIR / f"{batch_name}_file_info.json"
    if not meta_path.exists():
        print(f"âš ï¸  Meta-fichier introuvable : {meta_path}")
        return
    meta = json.loads(meta_path.read_text(encoding="utf-8"))
    file_id = meta["file_id"]

    # CrÃ©ation du batch (rÃ©servation des tokens)
    batch = client.batches.create(
        input_file_id=file_id,
        endpoint="/v1/responses",
        completion_window="24h",
        metadata={"batch_name": batch_name}
    )
    print(f"ğŸš€ Batch lancÃ© â€” id {batch.id}")
    # wait(batch.id)

# â”€â”€ SCRIPT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    if len(sys.argv) < 2:
        print("Usage: python launch_batches.py <batch_name1> [batch_name2 ...]")
        sys.exit(1)

    for name in sys.argv[1:]:
        launch_one(name)

if __name__ == "__main__":
    main()
