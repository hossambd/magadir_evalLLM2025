#!/usr/bin/env python3
"""
DÃ©coupe les prompts en lots, gÃ©nÃ¨re les .jsonl et
uploade chaque fichier sur OpenAI (purpose="batch").
Ne crÃ©e PAS les batchs : chaque lot reste prÃªt Ã  Ãªtre lancÃ©.
"""

import json
from pathlib import Path
from tqdm import tqdm
import tiktoken
from openai import OpenAI

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL = "gpt-4.1"
TOKEN_LIMIT_PER_BATCH = 1_200_000   # marge
PROMPT_ROOT_DIR = Path("event/generated_prompts/events")
OUTPUT_DIR = Path("event/openai_outputs")
BATCH_INPUT_DIR = OUTPUT_DIR / "batch_inputs"

# â”€â”€ INIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    ENCODING = tiktoken.encoding_for_model(MODEL)
except KeyError:
    ENCODING = tiktoken.get_encoding("o200k_base")

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
BATCH_INPUT_DIR.mkdir(parents=True, exist_ok=True)
client = OpenAI()

# â”€â”€ SCHÃ‰MA JSON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EVENT_SCHEMA = {
    "type": "object",
    "properties": {
        "events": {
            "type": "array",
            "items": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "attribute": {"type": "string"},
                        "value": {"type": "string"}
                    },
                    "required": ["attribute", "value"],
                    "additionalProperties": False
                }
            }
        }
    },
    "required": ["events"],
    "additionalProperties": False
}

# â”€â”€ UTILS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def count_tokens(text: str) -> int:
    return len(ENCODING.encode(text))

def build_batch_request(prompt_text: str, prompt_id: str) -> dict:
    return {
        "custom_id": prompt_id,
        "method": "POST",
        "url": "/v1/responses",
        "body": {
            "model": MODEL,
            "input": [
                {"role": "system",
                "content": "Tu es un assistant d'extraction d'Ã©vÃ©nements. Tu dois retourner un JSON de la forme : {\"events\": [[{\"attribute\": ..., \"value\": ...}, ...], ...]}"
                    },
                {"role": "user", "content": prompt_text}
            ],
            "text": {"format": {
                    "type": "json_schema",
                    "name": "event_response",
                    "schema": EVENT_SCHEMA,
                    "strict": True
                }}
        }
    }

# â”€â”€ TRAITEMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_one_strategy(strategy_dir: Path):
    strategy = strategy_dir.name
    prompts  = sorted(strategy_dir.glob("prompt_*.txt"))
    print(f"\nğŸ“‚ {strategy}: {len(prompts)} prompts trouvÃ©s")

    batches, cur_batch, token_sum = [], [], 0
    for pf in tqdm(prompts, desc=f"DÃ©coupage {strategy}"):
        txt   = pf.read_text(encoding="utf-8")
        req   = build_batch_request(txt, pf.stem)
        tokens = count_tokens(txt)
        if token_sum + tokens > TOKEN_LIMIT_PER_BATCH:
            batches.append(cur_batch)
            cur_batch, token_sum = [], 0
        cur_batch.append(req)
        token_sum += tokens
    if cur_batch:
        batches.append(cur_batch)

    print(f"ğŸ”¢ {len(batches)} lot(s) gÃ©nÃ©rÃ©(s)")

    for idx, lines in enumerate(batches, start=1):
        batch_name = f"{strategy}_part{idx}"
        jsonl_path = BATCH_INPUT_DIR / f"{batch_name}.jsonl"

        # 1. Ã‰criture
        with open(jsonl_path, "w", encoding="utf-8") as f:
            for line in lines:
                json.dump(line, f, ensure_ascii=False)
                f.write("\n")
        print(f"ğŸ“„ {jsonl_path} Ã©crit")

        # 2. Upload (mais pas de batch !)
        file_obj = client.files.create(file=open(jsonl_path, "rb"),
                                       purpose="batch")
        print(f"ğŸ“¤ Upload OK â€” file_id {file_obj.id}")

        # 3. Sauvegarde meta (servira Ã  launch_batches.py)
        meta_path = OUTPUT_DIR / f"{batch_name}_file_info.json"
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump({
                "batch_name": batch_name,
                "file_id": file_obj.id,
                "strategy": strategy,
                "part": idx
            }, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Meta sauvegardÃ© â†’ {meta_path}")

def main():
    dirs = [d for d in PROMPT_ROOT_DIR.iterdir() if d.is_dir()]
    if not dirs:
        print("âŒ Aucun dossier dans generated_prompts/")
        return
    for d in dirs:
        prepare_one_strategy(d)

if __name__ == "__main__":
    main()
